# Lexing of Inko source code into tokens.
#
# The types and methods of this module are not part of the public API at this
# time, meaning they can change at any time.
import std::byte_array::ToByteArray
import std::compiler::source_location::SourceLocation
import std::compiler::token::*
import std::conversion::ToString
import std::fs::path::(IntoPath, Path)
import std::map::Map
import std::option::Option
import std::range::Range

let INTEGER_DIGIT_RANGE = 48..57
let HEX_LOWER_DIGIT_RANGE = 97..102
let HEX_UPPER_DIGIT_RANGE = 65..70
let LOWER_AZ_RANGE = 97..122
let UPPER_AZ_RANGE = 65..90

let NULL = 0
let TAB = 9
let NEWLINE = 10
let CARRIAGE_RETURN = 13
let ESCAPE = 27
let SPACE = 32
let EXCLAMATION = 33
let DOUBLE_QUOTE = 34
let HASH = 35
let PERCENT = 37
let AMPERSAND = 38
let SINGLE_QUOTE = 39
let PAREN_OPEN = 40
let PAREN_CLOSE = 41
let STAR = 42
let PLUS = 43
let COMMA = 44
let MINUS = 45
let DOT = 46
let SLASH = 47
let ZERO = 48
let COLON = 58
let LOWER = 60
let EQUAL = 61
let GREATER = 62
let QUESTION = 63
let AT_SIGN = 64
let UPPER_E = 69
let UPPER_X = 88
let BRACKET_OPEN = 91
let BACKSLASH = 92
let BRACKET_CLOSE = 93
let CARET = 94
let UNDERSCORE = 95
let BACKTICK = 96
let LOWER_E = 101
let LOWER_N = 110
let LOWER_R = 114
let LOWER_T = 116
let LOWER_X = 120
let CURLY_OPEN = 123
let PIPE = 124
let CURLY_CLOSE = 125
let TILDE = 126

# The default state of the lexer. In this state, code is lexer as you'd expect.
let STATE_DEFAULT = 0

# The state used when lexing a template string.
let STATE_TEMPLATE_STRING = 1

# The maximum value in a `ByteArray`.
let MAX_BYTE = 255

# A type used to map bytes used in escape sequences to the bytes that replace
# them.
#
# An `EscapeMap` can only map bytes up to byte 128.
class EscapeMap {
  # A ByteArray that maps bytes to the ones they should be replaced with.
  #
  # Since NULL bytes are valid replacements, the value 255 is used to signal no
  # replacement exists.
  @mapping: ByteArray

  static def new -> Self {
    Self { @mapping = ByteArray.filled(amount: 128, value: MAX_BYTE) }
  }

  move def map(byte: Int, to: Int) -> Self {
    @mapping[byte] = to
    self
  }

  def get(input: Int) -> ?Int {
    let replacement = @mapping[input]

    if replacement < MAX_BYTE { Option.some(replacement) } else { Option.none }
  }
}

# A `Lexer` is used for turning Inko source code into a sequence of tokens.
# These tokens in turn can be used by a parser to produce an Abstract Syntax
# Tree.
class Lexer {
  # The byte stream to lex.
  @input: ByteArray

  # The file path that produced the byte stream.
  @file: Path

  # The maximum byte position in the input stream.
  @max_position: Int

  # The current byte position in the input stream.
  @position: Int

  # The number of opening curly braces that have yet to be closed.
  @curly_braces: Int

  # The stack of curly brace counts to use for determining when a template
  # string expression should be closed.
  @curly_brace_stack: Array!(Int)

  # The stack of lexing states.
  @states: Array!(Int)

  # The current line number.
  @line: Int

  # The current column number.
  @column: Int

  # The escape sequence literals supported by a single quoted string, and their
  # replacement bytes.
  @single_quoted_string_escapes: EscapeMap

  # The escape sequence literals supported by a double quoted string, and their
  # replacement bytes.
  @double_quoted_string_escapes: EscapeMap

  # The escape sequence literals supported by a template string string, and
  # their replacement bytes.
  @template_string_escapes: EscapeMap

  static def new(input: ToByteArray, file: IntoPath) -> Self {
    let bytes = input.to_byte_array
    let max = bytes.length

    Self {
      @input = bytes,
      @file = file.into_path,
      @max_position = max,
      @position = 0,
      @curly_braces = 0,
      @curly_brace_stack = Array.new,
      @states = Array.new(STATE_DEFAULT),
      @line = 1,
      @column = 1,
      @single_quoted_string_escapes = EscapeMap
        .new
        .map(byte: SINGLE_QUOTE, to: SINGLE_QUOTE)
        .map(byte: BACKSLASH, to: BACKSLASH),
      @double_quoted_string_escapes = EscapeMap
        .new
        .map(byte: DOUBLE_QUOTE, to: DOUBLE_QUOTE)
        .map(byte: SINGLE_QUOTE, to: SINGLE_QUOTE)
        .map(byte: ZERO, to: NULL)
        .map(byte: BACKSLASH, to: BACKSLASH)
        .map(byte: LOWER_E, to: ESCAPE)
        .map(byte: LOWER_N, to: NEWLINE)
        .map(byte: LOWER_R, to: CARRIAGE_RETURN)
        .map(byte: LOWER_T, to: TAB),
      @template_string_escapes = EscapeMap
        .new
        .map(byte: DOUBLE_QUOTE, to: DOUBLE_QUOTE)
        .map(byte: SINGLE_QUOTE, to: SINGLE_QUOTE)
        .map(byte: ZERO, to: NULL)
        .map(byte: BACKSLASH, to: BACKSLASH)
        .map(byte: BACKTICK, to: BACKTICK)
        .map(byte: LOWER_E, to: ESCAPE)
        .map(byte: LOWER_N, to: NEWLINE)
        .map(byte: LOWER_T, to: TAB)
        .map(byte: LOWER_R, to: CARRIAGE_RETURN)
        .map(byte: CURLY_OPEN, to: CURLY_OPEN)
    }
  }

  def current_position -> Int {
    @position.clone
  }

  def current_column -> Int {
    @column.clone
  }

  def current_line -> Int {
    @line.clone
  }

  def source_location(line: Int, column: Int) -> SourceLocation {
    SourceLocation.new(
      file: @file.clone,
      line_range: line..current_line,
      column: column
    )
  }

  def current_location -> SourceLocation {
    source_location(line: current_line, column: current_column)
  }

  # Returns `True` if there is input left to process.
  def next? -> Boolean {
    @position < @max_position
  }

  # Returns the next `Token` available, if any.
  def next -> Token {
    consume_whitespace

    let token = match @states[-1] {
      STATE_TEMPLATE_STRING -> { next_template_string_token }
      else -> { next_non_whitespace_token }
    }

    # We have to consume trailing whitespace. Not doing so may result in a
    # consumer of the iterator thinking there are tokens left to produce, when
    # this might not be the case.
    consume_whitespace

    token
  }

  def next_non_whitespace_token -> Token {
    match current_byte {
      INTEGER_DIGIT_RANGE -> { number(skip_after_start: False) }
      AT_SIGN -> { attribute }
      HASH -> { comment }
      CURLY_OPEN -> { curly_open }
      CURLY_CLOSE -> { curly_close }
      PAREN_OPEN -> { single_character_token('paren_open') }
      PAREN_CLOSE -> { single_character_token('paren_close') }
      SINGLE_QUOTE -> {
        string(
          quote: SINGLE_QUOTE,
          replacements: @single_quoted_string_escapes
        )
      }
      DOUBLE_QUOTE -> {
        string(
          quote: DOUBLE_QUOTE,
          replacements: @double_quoted_string_escapes
        )
      }
      COLON -> { colon }
      SLASH -> { single_operator(type: 'div', assign_type: 'div_assign') }
      PERCENT -> { percent }
      CARET -> { single_operator(type: 'xor', assign_type: 'xor_assign') }
      AMPERSAND -> { single_operator(type: 'and', assign_type: 'and_assign') }
      PIPE -> { single_operator(type: 'or', assign_type: 'or_assign') }
      STAR -> { star }
      MINUS -> { minus }
      PLUS -> { single_operator(type: 'add', assign_type: 'add_assign') }
      EQUAL -> { assign }
      LOWER -> { lower }
      GREATER -> { greater }
      BRACKET_OPEN -> { single_character_token('bracket_open') }
      BRACKET_CLOSE -> { single_character_token('bracket_close') }
      EXCLAMATION -> { exclamation }
      DOT -> { dot }
      COMMA -> { single_character_token('comma') }
      QUESTION -> { question }
      UNDERSCORE -> { underscore }
      LOWER_AZ_RANGE -> { identifier_or_keyword }
      UPPER_AZ_RANGE -> { constant }
      BACKTICK -> { template_string_open }
      else -> {
        if next? { invalid } else { NullToken.new(current_location) }
      }
    }
  }

  def next_template_string_token -> Token {
    match current_byte {
      BACKTICK -> { return template_string_close }
      CURLY_OPEN -> { return template_string_expression_open }
      NULL -> { return NullToken.new(current_location) }
      else -> { return template_string_text }
    }

    next_template_string_token
  }

  def number(skip_after_start: Boolean) -> Token {
    let mut first = current_byte
    let mut second = next_byte

    # When lexing negative numbers (e.g. -10) we want to skip the first
    # character. This also means we need to peek one byte further, instead of
    # just looking at the next one.
    if skip_after_start {
      first = next_byte
      second = peek(offset: 2)
    }

    # "0x" is only valid at the start of an integer.
    if INTEGER_DIGIT_RANGE.start == first
      and (second == LOWER_X or second == UPPER_X)
    {
      hexadecimal_integer(skip_after_start)
    } else {
      integer_or_float(skip_after_start)
    }
  }

  def integer_or_float(skip_after_start: Boolean) -> Token {
    let start = current_position
    let line = current_line
    let mut type = 'integer'

    if skip_after_start { @position += 1 }

    loop {
      match let byte = current_byte {
        INTEGER_DIGIT_RANGE, UNDERSCORE -> { True }
        LOWER_E, UPPER_E when INTEGER_DIGIT_RANGE =~ next_byte -> {
          type = 'float'
        }
        # If there is an exponent followed by "+" or "-", it's only valid if
        # also followed by a digit. This means "10e+" is invalid, and "10e+2"
        # is valid.
        LOWER_E, UPPER_E
        when next_byte == PLUS
          or (next_byte == MINUS)
          and (INTEGER_DIGIT_RANGE =~ peek(offset: 2))
        -> {
          @position += 1
          type = 'float'
        }
        DOT when INTEGER_DIGIT_RANGE =~ next_byte -> { type = 'float' }
        else -> { break }
      }

      @position += 1
    }

    token(type: type, start: start, line: line)
  }

  def hexadecimal_integer(skip_after_start: Boolean) -> Token {
    let start = current_position
    let line = current_line

    if skip_after_start { @position += 1 }

    # Advance 2 for "0x"
    @position += 2

    loop {
      match current_byte {
        INTEGER_DIGIT_RANGE,
        HEX_LOWER_DIGIT_RANGE,
        HEX_UPPER_DIGIT_RANGE,
        UNDERSCORE
        -> { @position += 1 }
        else -> { break }
      }
    }

    token(type: 'integer', start: start, line: line)
  }

  def comment -> Token {
    let column = current_column

    advance_one_byte

    let start = current_position

    while next? and not newline? { @position += 1 }

    let comment = token_with_column(
      type: 'comment',
      start: start,
      line: current_line,
      column: column
    )

    advance_line
    comment
  }

  def attribute -> Token {
    let start = current_position
    let line = current_line

    # Advance 1 for the @ sign itself.
    @position += 1

    while valid_identifier_byte? { @position += 1 }

    token(type: 'attribute', start: start, line: line)
  }

  def single_character_token(type: String) -> Token {
    let start = current_position

    @position += 1
    token(type: type, start: start, line: current_line)
  }

  def two_character_token(type: String) -> Token {
    let start = current_position

    @position += 2
    token(type: type, start: start, line: current_line)
  }

  def operator(type: String, assign_type: String, start: Int) -> Token {
    if next_byte == EQUAL {
      @position += 2

      return binary_assign_token(assign_type, start, line: current_line)
    }

    @position += 1

    binary_token(type, start, line: current_line)
  }

  # An operator consisting of a single token, such as `+`.
  def single_operator(type: String, assign_type: String) -> Token {
    operator(type, assign_type, start: current_position)
  }

  # An operator consisting of at least two tokens, such as `**`.
  def double_operator(type: String, assign_type: String) -> Token {
    let start = current_position

    @position += 1

    operator(type, assign_type, start)
  }

  def binary_operator(type: String, equality_type: String) -> Token {
    let start = current_position
    let token_type = if next_byte == EQUAL {
      @position += 2

      equality_type
    } else {
      @position += 1

      type
    }

    binary_token(type: token_type, start: start, line: current_line)
  }

  def string(quote: Int, replacements: ref EscapeMap) -> Token {
    let column = current_column
    let line = current_line
    let buffer = ByteArray.new
    let line_buffer = ByteArray.new

    # Advance for the opening quote.
    advance_one_byte

    while current_byte != quote and next? {
      append_string_byte(
        buffer: buffer,
        line_buffer: line_buffer,
        replacements: replacements
      )
    }

    # Advance for the closing quote.
    advance_one_byte

    let location = source_location(line: line, column: column)
    let value =
      string_value(buffer: buffer, line_buffer: line_buffer, start_line: line)

    RegularToken.new(type: 'string', value: value, location: location)
  }

  def append_string_byte(
    buffer: ref ByteArray,
    line_buffer: ref ByteArray,
    replacements: ref EscapeMap
  ) -> Boolean {
    let current = current_byte

    if current == BACKSLASH
      and append_escaped_string_byte(buffer, line_buffer, replacements)
    {
      return True
    }

    buffer.push(current)
    line_buffer.push(current)

    if newline? {
      advance_line
      line_buffer.clear
    } else {
      @position += 1
    }

    False
  }

  def append_escaped_string_byte(
    buffer: ref ByteArray,
    line_buffer: ref ByteArray,
    replacements: ref EscapeMap
  ) -> Boolean {
    let replace_with = try replacements.get(next_byte).get else return False

    buffer.push(replace_with)
    line_buffer.push(replace_with)

    @position += 2
    @column += 1

    True
  }

  def string_value(
    buffer: ByteArray,
    line_buffer: ByteArray,
    start_line: Int
  ) -> String {
    let value = buffer.drain_to_string

    # If a string contains newlines, we only want to increment the column by the
    # length of the last line.
    #
    # To prevent allocating an additional String, we only use the line buffer if
    # needed.
    let advance_column_by = if @line > start_line {
      line_buffer.drain_to_string.length
    } else {
      value.length
    }

    @column += advance_column_by

    value
  }

  def curly_open -> Token {
    @curly_braces += 1
    single_character_token('curly_open')
  }

  def curly_close -> Token {
    if @curly_braces.positive? { @curly_braces -= 1 }

    let count = try @curly_brace_stack.get(-1).get else -1

    if count == @curly_braces {
      @curly_brace_stack.pop
      @states.pop
      single_character_token('tstring_expr_close')
    } else {
      single_character_token('curly_close')
    }
  }

  def template_string_open -> Token {
    @states.push(STATE_TEMPLATE_STRING)
    single_character_token('tstring_open')
  }

  def template_string_close -> Token {
    @states.pop
    single_character_token('tstring_close')
  }

  def template_string_expression_open -> Token {
    @states.push(STATE_DEFAULT)
    @curly_brace_stack.push(@curly_braces.clone)

    @curly_braces += 1

    single_character_token('tstring_expr_open')
  }

  def template_string_text -> Token {
    let column = current_column
    let line = current_line
    let buffer = ByteArray.new
    let line_buffer = ByteArray.new

    loop {
      # Template strings may be wrapped across lines with a backslash, like
      # so:
      #
      #     `foo \
      #       bar`
      #
      # In this case, the backslash and all whitespace that follows it should
      # be skipped. This means the above is equivalent to:
      #
      #     `foo bar`
      if current_byte == BACKSLASH and next_is_whitespace? {
        advance_one_byte
        consume_whitespace
      }

      match current_byte {
        NULL, BACKTICK, CURLY_OPEN -> { break }
        else -> {}
      }

      append_string_byte(
        buffer: buffer,
        line_buffer: line_buffer,
        replacements: @template_string_escapes
      )
    }

    let location = source_location(line: line, column: column)
    let value =
      string_value(buffer: buffer, line_buffer: line_buffer, start_line: line)

    RegularToken.new(type: 'tstring_text', value: value, location: location)
  }

  def colon -> Token {
    let start = current_position
    let type = if next_byte == COLON {
      @position += 2

      'colon_colon'
    } else {
      @position += 1

      'colon'
    }

    token(type, start, line: current_line)
  }

  def percent -> Token {
    single_operator(type: 'mod', assign_type: 'mod_assign')
  }

  def star -> Token {
    if next_byte == STAR {
      return double_operator(
        type: 'pow',
        assign_type: 'pow_assign',
      )
    }

    single_operator(type: 'mul', assign_type: 'mul_assign')
  }

  def minus -> Token {
    match next_byte {
      INTEGER_DIGIT_RANGE -> { number(skip_after_start: True) }
      GREATER -> { arrow }
      else -> { single_operator(type: 'sub', assign_type: 'sub_assign') }
    }
  }

  def assign -> Token {
    let start = current_position
    let line = current_line

    match next_byte {
      EQUAL -> {
        @position += 2
        binary_token(type: 'equal', start: start, line: line)
      }
      TILDE -> {
        @position += 2
        binary_token(type: 'match', start: start, line: line)
      }
      GREATER -> {
        @position += 2
        token(type: 'double_arrow', start: start, line: line)
      }
      else -> {
        @position += 1
        token(type: 'assign', start: start, line: line)
      }
    }
  }

  def lower -> Token {
    if next_byte == LOWER {
      return double_operator(
        type: 'shift_left',
        assign_type: 'shift_left_assign',
      )
    }

    binary_operator(type: 'lower', equality_type: 'lower_equal')
  }

  def greater -> Token {
    if next_byte == GREATER {
      return double_operator(
        type: 'shift_right',
        assign_type: 'shift_right_assign',
      )
    }

    binary_operator(type: 'greater', equality_type: 'greater_equal')
  }

  def not_equal -> Token {
    let start = current_position

    @position += 2

    binary_token(type: 'not_equal', start: start, line: current_line)
  }

  def arrow -> Token {
    let start = current_position

    @position += 2

    token(type: 'arrow', start: start, line: current_line)
  }

  def exclamation -> Token {
    match next_byte {
      EQUAL -> { not_equal }
      PAREN_OPEN -> { two_character_token('type_args_open') }
      EXCLAMATION -> { two_character_token('throws') }
      else -> { single_character_token('exclamation') }
    }
  }

  def dot -> Token {
    let start = current_position

    @position += 1

    if current_byte == DOT and next_byte == DOT {
      @position += 2

      return binary_token(
        type: 'exclusive_range',
        start: start,
        line: current_line
      )
    }

    if current_byte == DOT {
      @position += 1

      return binary_token(
        type: 'inclusive_range',
        start: start,
        line: current_line
      )
    }

    token(type: 'dot', start: start, line: current_line)
  }

  def question -> Token {
    let start = current_position

    @position += 1

    if current_byte == QUESTION {
      @position += 1

      return binary_token(
        type: 'question_question',
        start: start,
        line: current_line
      )
    }

    token(type: 'question', start: start, line: current_line)
  }

  def underscore -> Token {
    let start = current_position

    while current_byte == UNDERSCORE { @position += 1 }

    if UPPER_AZ_RANGE.cover?(current_byte) {
      return identifier_or_constant(type: 'constant', start: start)
    }

    identifier_or_constant(type: 'identifier', start: start)
  }

  def identifier_or_keyword -> Token {
    let start = current_position

    while valid_identifier_byte? { @position += 1 }

    let string = slice_string(start: start, stop: current_position)
    let location = source_location(line: current_line, column: current_column)

    @column += string.length

    match string {
      'as',
      'class',
      'def',
      'do',
      'else',
      'extern',
      'fn',
      'for',
      'impl',
      'import',
      'let',
      'local',
      'match',
      'move',
      'mut',
      'ref',
      'return',
      'self',
      'static',
      'throw',
      'trait',
      'try',
      'when',
      'yield' -> {
        KeywordToken.new(keyword: string, location: location)
      }
      else -> {
        RegularToken.new(type: 'identifier', value: string, location: location)
      }
    } as Token
  }

  def constant -> Token {
    identifier_or_constant(type: 'constant', start: current_position)
  }

  def identifier_or_constant(type: String, start: Int) -> Token {
    while valid_identifier_byte? { @position += 1 }
    token(type: type, start: start, line: current_line)
  }

  def token_with_column(
    type: String,
    start: Int,
    line: Int,
    column: Int
  ) -> Token {
    let value = slice_string(start: start, stop: current_position)
    let location = source_location(line: line, column: column)

    @column += value.length

    RegularToken.new(type: type, value: value, location: location)
  }

  def token(type: String, start: Int, line: Int) -> Token {
    token_with_column(type, start, line, current_column)
  }

  def binary_token(type: String, start: Int, line: Int) -> Token {
    let value = slice_string(start: start, stop: current_position)
    let location = source_location(line: line, column: current_column)

    @column += value.length

    BinaryToken.new(type: type, value: value, location: location)
  }

  def binary_assign_token(type: String, start: Int, line: Int) -> Token {
    let value = slice_string(start: start, stop: current_position)
    let location = source_location(line: line, column: current_column)

    @column += value.length

    BinaryAssignToken.new(type: type, value: value, location: location)
  }

  def invalid -> Token {
    let value = slice_string(start: current_position, stop: @position + 1)
    let location = source_location(line: current_line, column: current_column)

    # When we run into invalid input we want to immediately stop processing any
    # further input.
    @position = @max_position.clone

    InvalidToken.new(value: value, location: location)
  }

  # Consumes any whitespace, until reaching the first non-whitespace character.
  def consume_whitespace {
    match current_byte {
      SPACE, TAB, CARRIAGE_RETURN -> {
        @position += 1
        @column += 1
      }
      NEWLINE -> { advance_line }
      else -> { return }
    }

    consume_whitespace
  }

  def next_is_whitespace? -> Boolean {
    match next_byte {
      SPACE, TAB, CARRIAGE_RETURN, NEWLINE -> { True }
      else -> { False }
    }
  }

  def advance_line {
    @position += 1
    @column = 1
    @line += 1
  }

  def advance_one_byte {
    @position += 1
    @column += 1
  }

  def slice_string(start: Int, stop: Int) -> String {
    @input.slice(start: start, length: stop - start).drain_to_string
  }

  def current_byte -> Int {
    if next? { @input[@position] } else { 0 }
  }

  def next_byte -> Int {
    peek(offset: 1)
  }

  def peek(offset: Int) -> Int {
    let index = @position + offset

    if index < @max_position { @input[index] } else { 0 }
  }

  def valid_identifier_byte? -> Boolean {
    match current_byte {
      INTEGER_DIGIT_RANGE, LOWER_AZ_RANGE, UPPER_AZ_RANGE -> { True }
      UNDERSCORE, QUESTION -> { True }
      else -> { False }
    }
  }

  def newline? -> Boolean {
    current_byte == NEWLINE
  }
}
